{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "SENTIMENTS = {\n",
    "        'POSITIVE': 1,\n",
    "        'NEGATIVE': -1,\n",
    "        'NEUTRAL': 0,\n",
    "        'UNKNOWN': 2\n",
    "    }\n",
    "\n",
    "CREDENTIALS = {\n",
    "    'host': 'ec2-50-17-207-16.compute-1.amazonaws.com',\n",
    "    'database': 'dcr3mmv32spva8',\n",
    "    'user': 'xobfxapscvkbir',\n",
    "    'port': '5432',\n",
    "    'pwd': '33487c61b69420cff89330da6fe2b82b280daabff756e82744a93cc4ddfc03f6',\n",
    "    'uri': 'postgres://xobfxapscvkbir:33487c61b69420cff89330da6fe2b82b280daabff756e82744a93cc4ddfc03f6@ec2-50-17-207-16.compute-1.amazonaws.com:5432/dcr3mmv32spva8',\n",
    "    'cli': 'heroku pg:psql postgresql-flat-49197 --app beepper'\n",
    "}\n",
    "\n",
    "COLUMNS = ['id','txt']\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        database=CREDENTIALS.get('database'),\n",
    "        user=CREDENTIALS.get('user'),\n",
    "        password=CREDENTIALS.get('pwd'),\n",
    "        host=CREDENTIALS.get('host'),\n",
    "        port=CREDENTIALS.get('port'),\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Connection to database failed\")\n",
    "\n",
    "curr = conn.cursor()\n",
    "curr.execute(\"SELECT id, txt_clean FROM hello_question\")\n",
    "res = curr.fetchall()\n",
    "\n",
    "curr.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(res,columns=COLUMNS).reset_index(drop=True)\n",
    "# data.to_csv('data/extract_23_02.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "class Matcher:\n",
    "    def __init__(self, num_match_returned=5):\n",
    "        self.num_match_returned = num_match_returned\n",
    "        self.stpwds = nltk.corpus.stopwords.words(\"french\")\n",
    "        self.vectorizer = None\n",
    "        self.dtm = None\n",
    "        self.questions_id = None\n",
    "        # rajouter des mots commes les, docteur, ...\n",
    "\n",
    "    def tokenize(self, content):\n",
    "        if content is None:\n",
    "            return None\n",
    "        else:\n",
    "            #remove http links\n",
    "            content = re.sub(r'http\\S+', '', content)\n",
    "            # remove formatting\n",
    "            content = re.sub(\"\\s+\", \" \", content)\n",
    "            # convert to lower case\n",
    "            content = content.lower()\n",
    "            # Remove accent\n",
    "            content = unidecode(content)\n",
    "            # remove punctuation (preserving intra-word dashes)\n",
    "            # content = \"\".join(letter for letter in content if letter not in punct)\n",
    "            punct = string.punctuation.replace(\"-\", \"\")\n",
    "            regex = re.compile('[%s]' % re.escape(punct))\n",
    "            content = regex.sub(' ', content)\n",
    "            content = re.sub(\"[^a-zA-Z]\", \" \", content)\n",
    "            # remove dashes attached to words but that are not intra-word\n",
    "            content = re.sub(\"[^[:alnum:]['-]\", \" \", content)\n",
    "            content = re.sub(\"[^[:alnum:][-']\", \" \", content)\n",
    "\n",
    "            # remove extra white space\n",
    "            content = re.sub(\" +\", \" \", content)\n",
    "            # remove leading and trailing white space\n",
    "            content = content.strip()\n",
    "            # tokenize\n",
    "            tokens = content.split(\" \")\n",
    "            # remove stopwords\n",
    "            tokens = [token for token in tokens if token not in self.stpwds and len(token) > 2]\n",
    "            return tokens\n",
    "        \n",
    "    def tokenize_dataset(self, data):\n",
    "        tokens = []\n",
    "        l = data.txt\n",
    "        if isinstance(l,str):\n",
    "            l = [l]\n",
    "        tokens = [self.tokenize(q) for q in l]\n",
    "        return tokens\n",
    "\n",
    "    def train(self, X):\n",
    "        tokens = self.tokenize_dataset(X)\n",
    "        voc = list(set([i for sublist in tokens if sublist is not None for i in sublist]))\n",
    "        join_tokens = [\" \".join(i) for i in tokens]\n",
    "        self.vectorizer = CountVectorizer(vocabulary=voc)\n",
    "        self.dtm = self.vectorizer.fit_transform(join_tokens)\n",
    "        return True\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.vectorizer is None:\n",
    "            self.train(X)\n",
    "        query_tokens = self.tokenize(query)\n",
    "        join_tokens = \" \".join(query_tokens)\n",
    "        transf = self.vectorizer.transform([join_tokens])\n",
    "        return transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(634, 3014)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Matcher()\n",
    "m.train(data)\n",
    "m.dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
