{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "SENTIMENTS = {\n",
    "        'POSITIVE': 1,\n",
    "        'NEGATIVE': -1,\n",
    "        'NEUTRAL': 0,\n",
    "        'UNKNOWN': 2\n",
    "    }\n",
    "\n",
    "CREDENTIALS = {\n",
    "    'host': 'ec2-50-17-207-16.compute-1.amazonaws.com',\n",
    "    'database': 'XXX',\n",
    "    'user': 'XXX',\n",
    "    'port': '5432',\n",
    "    'pwd': 'XXXXX',\n",
    "    'uri': 'postgres://XXX@host:5432/XX',\n",
    "    'cli': 'heroku pg:psql postgresql-flat-49197 --app beepper'\n",
    "}\n",
    "\n",
    "COLUMNS = ['id','txt']\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        database=CREDENTIALS.get('database'),\n",
    "        user=CREDENTIALS.get('user'),\n",
    "        password=CREDENTIALS.get('pwd'),\n",
    "        host=CREDENTIALS.get('host'),\n",
    "        port=CREDENTIALS.get('port'),\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Connection to database failed\")\n",
    "\n",
    "curr = conn.cursor()\n",
    "curr.execute(\"SELECT id, txt_clean FROM hello_question\")\n",
    "res = curr.fetchall()\n",
    "\n",
    "curr.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(res,columns=COLUMNS).reset_index(drop=True)\n",
    "# data.to_csv('data/extract_23_02.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = {'médicaments': [1012, 994, 974, 919, 913, 801, 776, 770, 760, 733],\n",
    "'administratif': [1015, 1013, 1011, 1009, 986, 984, 981, 980, 979, 918],\n",
    "'pratique médicale': [1001,891,942,934,965,926,968,1000,895,864], \n",
    "'diagnostique' : [989,987,985,964,957,951,950,937,903,872]}\n",
    "\n",
    "data['group'] = None\n",
    "for g in groups.items():\n",
    "    for i in g[1]:\n",
    "        data.ix[data.id==i, 'group'] = g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# data.ix[data.id==919]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "class Matcher:\n",
    "    def __init__(self, num_match_returned=5):\n",
    "        self.num_match_returned = num_match_returned\n",
    "        self.stpwds = nltk.corpus.stopwords.words(\"french\")\n",
    "        self.vectorizer = None\n",
    "        self.dtm = None\n",
    "        self.questions_id = None\n",
    "        # rajouter des mots commes les, docteur, ...\n",
    "\n",
    "    def tokenize(self, content):\n",
    "        if content is None:\n",
    "            return None\n",
    "        else:\n",
    "            #remove http links\n",
    "            content = re.sub(r'http\\S+', '', content)\n",
    "            # remove formatting\n",
    "            content = re.sub(\"\\s+\", \" \", content)\n",
    "            # convert to lower case\n",
    "            content = content.lower()\n",
    "            # Remove accent\n",
    "            content = unidecode(content)\n",
    "            # remove punctuation (preserving intra-word dashes)\n",
    "            # content = \"\".join(letter for letter in content if letter not in punct)\n",
    "            punct = string.punctuation.replace(\"-\", \"\")\n",
    "            regex = re.compile('[%s]' % re.escape(punct))\n",
    "            content = regex.sub(' ', content)\n",
    "            content = re.sub(\"[^a-zA-Z]\", \" \", content)\n",
    "            # remove dashes attached to words but that are not intra-word\n",
    "            content = re.sub(\"[^[:alnum:]['-]\", \" \", content)\n",
    "            content = re.sub(\"[^[:alnum:][-']\", \" \", content)\n",
    "\n",
    "            # remove extra white space\n",
    "            content = re.sub(\" +\", \" \", content)\n",
    "            # remove leading and trailing white space\n",
    "            content = content.strip()\n",
    "            # tokenize\n",
    "            tokens = content.split(\" \")\n",
    "            # remove stopwords\n",
    "            tokens = [token for token in tokens if token not in self.stpwds and len(token) > 2]\n",
    "            return tokens\n",
    "        \n",
    "    def tokenize_dataset(self, data):\n",
    "        tokens = []\n",
    "        l = data.txt\n",
    "        if isinstance(l,str):\n",
    "            l = [l]\n",
    "        tokens = [self.tokenize(q) for q in l]\n",
    "        return tokens\n",
    "\n",
    "    def train(self, X):\n",
    "        tokens = self.tokenize_dataset(X)\n",
    "        voc = list(set([i for sublist in tokens if sublist is not None for i in sublist]))\n",
    "        join_tokens = [\" \".join(i) for i in tokens]\n",
    "        self.vectorizer = CountVectorizer(vocabulary=voc)\n",
    "        self.dtm = self.vectorizer.fit_transform(join_tokens)\n",
    "        return True\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.vectorizer is None:\n",
    "            self.train(X)\n",
    "        query_tokens = self.tokenize(query)\n",
    "        join_tokens = \" \".join(query_tokens)\n",
    "        transf = self.vectorizer.transform([join_tokens])\n",
    "        return transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = Matcher()\n",
    "m.train(data)\n",
    "X = m.dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['k-means 0', 'k-means 1', 'k-means 2', 'k-means 3'], dtype=object)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "init = []\n",
    "unique_labels = [i for i in data.group.unique() if i is not None]\n",
    "for u in unique_labels:\n",
    "    init.append(X.toarray()[data.group==u,:].mean(axis=0))\n",
    "init = np.array(init)\n",
    "\n",
    "est = KMeans(n_clusters=4)\n",
    "est.fit(X)\n",
    "labels = est.labels_\n",
    "unique_labels = ['k-means '+str(i) for i in np.unique(labels)]\n",
    "labels2 = np.zeros(labels.shape).astype(object)\n",
    "for u in np.unique(labels):\n",
    "    labels2[labels==u] = 'k-means '+str(u)\n",
    "np.unique(labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred       true             \n",
       "k-means 0  administratif        8\n",
       "           diagnostique         7\n",
       "           médicaments          9\n",
       "           pratique médicale    6\n",
       "k-means 1  administratif        2\n",
       "           médicaments          1\n",
       "           pratique médicale    3\n",
       "k-means 2  pratique médicale    1\n",
       "k-means 3  diagnostique         3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([data.group[~pd.isnull(data.group)].values,labels2[~pd.isnull(data.group)]]).transpose()\n",
    "df.columns = ['true','pred']\n",
    "df.groupby(by=['pred','true']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Evaluation of K-means clusters\n",
      "- k-means 0, Score: 0.2903225806451613\n",
      "Of 30 samples put in that cluster, 9 are of type médicaments\n",
      "- k-means 2, Score: 0.5\n",
      "Of 1 samples put in that cluster, 1 are of type pratique médicale\n",
      "- k-means 1, Score: 0.42857142857142855\n",
      "Of 6 samples put in that cluster, 3 are of type pratique médicale\n",
      "- k-means 3, Score: 0.75\n",
      "Of 3 samples put in that cluster, 3 are of type diagnostique\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df.groupby(by=['pred','true']).size()\n",
    "print('>>> Evaluation of K-means clusters')\n",
    "for i in df.pred.unique():\n",
    "    score = a.ix[i].max()/(1+a.ix[i].sum())\n",
    "    print(\"- {}, Score: {}\".format(i, score))\n",
    "    print(\"Of {} samples put in that cluster, {} are of type {}\".format(a.ix[i].sum(), a.ix[i].max(), a.ix[i].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
