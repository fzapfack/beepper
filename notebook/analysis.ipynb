{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "SENTIMENTS = {\n",
    "        'POSITIVE': 1,\n",
    "        'NEGATIVE': -1,\n",
    "        'NEUTRAL': 0,\n",
    "        'UNKNOWN': 2\n",
    "    }\n",
    "\n",
    "CREDENTIALS = {\n",
    "    'host': 'ec2-50-17-207-16.compute-1.amazonaws.com',\n",
    "    'database': 'dcr3mmv32spva8',\n",
    "    'user': 'xobfxapscvkbir',\n",
    "    'port': '5432',\n",
    "    'pwd': '33487c61b69420cff89330da6fe2b82b280daabff756e82744a93cc4ddfc03f6',\n",
    "    'uri': 'postgres://xobfxapscvkbir:33487c61b69420cff89330da6fe2b82b280daabff756e82744a93cc4ddfc03f6@ec2-50-17-207-16.compute-1.amazonaws.com:5432/dcr3mmv32spva8',\n",
    "    'cli': 'heroku pg:psql postgresql-flat-49197 --app beepper'\n",
    "}\n",
    "\n",
    "COLUMNS = ['id','txt']\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        database=CREDENTIALS.get('database'),\n",
    "        user=CREDENTIALS.get('user'),\n",
    "        password=CREDENTIALS.get('pwd'),\n",
    "        host=CREDENTIALS.get('host'),\n",
    "        port=CREDENTIALS.get('port'),\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Connection to database failed\")\n",
    "\n",
    "curr = conn.cursor()\n",
    "curr.execute(\"SELECT id, txt_clean FROM hello_question\")\n",
    "res = curr.fetchall()\n",
    "\n",
    "curr.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(res,columns=COLUMNS).reset_index(drop=True)\n",
    "# data.to_csv('data/extract_23_02.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = {'médicaments': [1012, 994, 974, 919, 913, 801, 776, 770, 760, 733],\n",
    "'administratif': [1015, 1013, 1011, 1009, 986, 984, 981, 980, 979, 918],\n",
    "'pratique médicale': [1001,891,942,934,965,926,968,1000,895,864], \n",
    "'diagnostique' : [989,987,985,964,957,951,950,937,903,872]}\n",
    "\n",
    "data['group'] = None\n",
    "for g in groups.items():\n",
    "    for i in g[1]:\n",
    "        data.ix[data.id==i, 'group'] = g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# data.ix[data.id==919]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "class Matcher:\n",
    "    def __init__(self, num_match_returned=5):\n",
    "        self.num_match_returned = num_match_returned\n",
    "        self.stpwds = nltk.corpus.stopwords.words(\"french\")\n",
    "        self.vectorizer = None\n",
    "        self.dtm = None\n",
    "        self.questions_id = None\n",
    "        # rajouter des mots commes les, docteur, ...\n",
    "\n",
    "    def tokenize(self, content):\n",
    "        if content is None:\n",
    "            return None\n",
    "        else:\n",
    "            #remove http links\n",
    "            content = re.sub(r'http\\S+', '', content)\n",
    "            # remove formatting\n",
    "            content = re.sub(\"\\s+\", \" \", content)\n",
    "            # convert to lower case\n",
    "            content = content.lower()\n",
    "            # Remove accent\n",
    "            content = unidecode(content)\n",
    "            # remove punctuation (preserving intra-word dashes)\n",
    "            # content = \"\".join(letter for letter in content if letter not in punct)\n",
    "            punct = string.punctuation.replace(\"-\", \"\")\n",
    "            regex = re.compile('[%s]' % re.escape(punct))\n",
    "            content = regex.sub(' ', content)\n",
    "            content = re.sub(\"[^a-zA-Z]\", \" \", content)\n",
    "            # remove dashes attached to words but that are not intra-word\n",
    "            content = re.sub(\"[^[:alnum:]['-]\", \" \", content)\n",
    "            content = re.sub(\"[^[:alnum:][-']\", \" \", content)\n",
    "\n",
    "            # remove extra white space\n",
    "            content = re.sub(\" +\", \" \", content)\n",
    "            # remove leading and trailing white space\n",
    "            content = content.strip()\n",
    "            # tokenize\n",
    "            tokens = content.split(\" \")\n",
    "            # remove stopwords\n",
    "            tokens = [token for token in tokens if token not in self.stpwds and len(token) > 2]\n",
    "            return tokens\n",
    "        \n",
    "    def tokenize_dataset(self, data):\n",
    "        tokens = []\n",
    "        l = data.txt\n",
    "        if isinstance(l,str):\n",
    "            l = [l]\n",
    "        tokens = [self.tokenize(q) for q in l]\n",
    "        return tokens\n",
    "\n",
    "    def train(self, X):\n",
    "        tokens = self.tokenize_dataset(X)\n",
    "        voc = list(set([i for sublist in tokens if sublist is not None for i in sublist]))\n",
    "        join_tokens = [\" \".join(i) for i in tokens]\n",
    "        self.vectorizer = CountVectorizer(vocabulary=voc)\n",
    "        self.dtm = self.vectorizer.fit_transform(join_tokens)\n",
    "        return True\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.vectorizer is None:\n",
    "            self.train(X)\n",
    "        query_tokens = self.tokenize(query)\n",
    "        join_tokens = \" \".join(query_tokens)\n",
    "        transf = self.vectorizer.transform([join_tokens])\n",
    "        return transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = Matcher()\n",
    "m.train(data)\n",
    "X = m.dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-78-ff8e39f8de7a>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-78-ff8e39f8de7a>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def eval_clustering(true,pred):\n",
    "    if len(np.unique(true)) != len(np.unique(pred)):\n",
    "        print(\"l'algorithme s'est trompe sur le nombre de groupes?\")\n",
    "        return 0\n",
    "    else:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3014"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fzapfack/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:889: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "init = []\n",
    "unique_labels = [i for i in data.group.unique() if i is not None]\n",
    "for u in unique_labels:\n",
    "    init.append(X.toarray()[data.group==u,:].mean(axis=0))\n",
    "init = np.array(init)\n",
    "\n",
    "est = KMeans(n_clusters=4, init=init)\n",
    "est.fit(X)\n",
    "labels = est.labels_\n",
    "pred, true = labels[~pd.isnull(data.group)], data.group[~pd.isnull(data.group)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred  true             \n",
       "0     administratif        7\n",
       "      diagnostique         6\n",
       "      médicaments          3\n",
       "      pratique médicale    6\n",
       "1     administratif        1\n",
       "      diagnostique         1\n",
       "      médicaments          6\n",
       "      pratique médicale    1\n",
       "2     administratif        2\n",
       "      médicaments          1\n",
       "      pratique médicale    3\n",
       "3     diagnostique         3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([true,pred]).transpose()\n",
    "df.columns = ['true','pred']\n",
    "df.groupby(by=['pred','true']).size()\n",
    "# df.groupby(by=['true','pred']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.6\n",
      "0\n",
      "0.304347826087\n",
      "2\n",
      "0.428571428571\n",
      "3\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "a = df.groupby(by=['pred','true']).size()\n",
    "for i in df.pred.unique():\n",
    "    print(i)\n",
    "    print(a.ix[i].max()/(1+a.ix[i].sum()))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
